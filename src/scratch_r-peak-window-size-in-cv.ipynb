{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x_/27ffh59s2pl353c1p0mccnzc0000gn/T/ipykernel_14290/1582698094.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_ecg_data_and_annotations(ecg_file_path, annotation_file_path):\n",
    "    ecg_data = pd.read_csv(ecg_file_path)\n",
    "    annotations = pd.read_csv(annotation_file_path)\n",
    "\n",
    "    return ecg_data, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def _butter_filter(sequence):\n",
    "    fs = 360  # Sampling frequency\n",
    "    nyquist = 0.5 * fs\n",
    "    low = 0.4 / nyquist\n",
    "    high = 45 / nyquist\n",
    "\n",
    "    b, a = butter(N=3, Wn=[low, high], btype='band')\n",
    "    return filtfilt(b, a, sequence)\n",
    "\n",
    "def apply_filter(ecg_data):\n",
    "    filtered_data = ecg_data.copy()\n",
    "    for lead in ['MLII', 'V1']:\n",
    "        # Check if the lead is in the DataFrame\n",
    "        if lead in ecg_data.columns:\n",
    "            filtered_data[lead] = _butter_filter(ecg_data[lead].values)\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def detect_r_peaks(ecg_lead, distance=180):\n",
    "    peaks, _ = find_peaks(ecg_lead, distance=distance)\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data_207, annotations_207 = load_ecg_data_and_annotations('../data/207/207.csv', '../data/207/207annotations.csv')\n",
    "filtered_ecg_data_207 = apply_filter(ecg_data_207)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data(filtered_ecg_data_207, annotations_207, window_size=180):\n",
    "    # Detect R-Peaks\n",
    "    # window_size = 180\n",
    "    r_peaks = detect_r_peaks(filtered_ecg_data_207['MLII'].values, distance=window_size)\n",
    "    \n",
    "    # Create Segmentations based on R-Peaks\n",
    "    segment_data = []\n",
    "    for r_peak in r_peaks:\n",
    "        start = max(0, r_peak - window_size // 2)\n",
    "        end = min(len(filtered_ecg_data_207), r_peak + window_size // 2)\n",
    "\n",
    "        relevant_annotations = annotations_207[(annotations_207['Sample #'] >= start) & (annotations_207['Sample #'] <= end)]\n",
    "        relevant_annotations = relevant_annotations[relevant_annotations['Type'].isin(['L', 'V', 'A', 'E', '!'])]\n",
    "\n",
    "        if not relevant_annotations.empty:\n",
    "            closest_annotation = relevant_annotations.iloc[(relevant_annotations['Sample #'] - r_peak).abs().argsort()[:1]]\n",
    "            label = closest_annotation['Type'].values[0]\n",
    "            segment_data.append({'Start': start, 'End': end, 'Label': label})\n",
    "\n",
    "    segments = pd.DataFrame(segment_data)\n",
    "    \n",
    "    # Remove last Row since window size < 180\n",
    "    segments.drop(segments.tail(1).index,inplace=True)\n",
    "\n",
    "\n",
    "    # Create arrays from DataFrame\n",
    "    segments_feature_1 = []\n",
    "    segments_feature_2 = []\n",
    "    segment_labels = []\n",
    "    for index, row in segments.iterrows():\n",
    "        start_index = int(row['Start'])\n",
    "        end_index = int(row['End'])\n",
    "        label = row['Label']\n",
    "        \n",
    "        segment_mlII = filtered_ecg_data_207['MLII'][start_index:end_index+1].values\n",
    "        segment_v1 = filtered_ecg_data_207['V1'][start_index:end_index+1].values\n",
    "        \n",
    "        segments_feature_1.append(segment_mlII)\n",
    "        segments_feature_2.append(segment_v1)\n",
    "        segment_labels.append(label)\n",
    "\n",
    "    combined_segments = [np.column_stack((mlII, v1)) for mlII, v1 in zip(segments_feature_1, segments_feature_2)]\n",
    "    combined_segments_array = np.array([np.array(segment) for segment in combined_segments], dtype=object)\n",
    "\n",
    "    label_mapping = {'L': 0, 'V': 1, 'A': 2, 'E': 3, '!': 4}\n",
    "    integer_labels = np.array([label_mapping[label] for label in segment_labels])\n",
    "    one_hot_labels = to_categorical(integer_labels)\n",
    "\n",
    "    print(f\"Combined Segments Shape: {combined_segments_array.shape}\")\n",
    "    print(f\"One-Hot Labels Shape: {one_hot_labels.shape}\")\n",
    "\n",
    "    # Split data into train/test\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        combined_segments_array, one_hot_labels, test_size=0.2, random_state=42, stratify=one_hot_labels\n",
    "    )\n",
    "\n",
    "    # Display Class Distribution\n",
    "    integer_labels_from_one_hot = np.argmax(train_y, axis=1)\n",
    "    class_counts = np.bincount(integer_labels_from_one_hot)\n",
    "    class_names = ['L', 'V', 'A', 'E', '!']\n",
    "    for class_name, count in zip(class_names, class_counts):\n",
    "        print(f\"Class {class_name}: {count}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Standardise Train Set\n",
    "    nsamples, ntimesteps, nfeatures = train_x.shape\n",
    "    train_x_reshaped = train_x.reshape((nsamples*ntimesteps, nfeatures))\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_x_reshaped)\n",
    "    train_x_standardised = scaler.transform(train_x_reshaped)\n",
    "    train_x_standardised = train_x_standardised.reshape((nsamples, ntimesteps, nfeatures))\n",
    "\n",
    "    return train_x_standardised, train_y, test_x, test_y, scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(results, metrics_names, metric_key):\n",
    "    for name, value in zip(metrics_names, results):\n",
    "        if metric_key in name:\n",
    "            return value\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_space = {\n",
    "    'window_size': [180, 280, 380],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "    'lstm_units': [32, 64],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'num_lstm_layers': [2, 3, 4],\n",
    "    'reg_learning_rate': [0.1, 0.01, 0.001]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_and_train_model(train_x, train_y, dropout_rate, lstm_units, batch_size, learning_rate, reg_learning_rate, num_lstm_layers, window_size, val_x=[], val_y=[]):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, return_sequences=True, input_shape=(train_x.shape[1], train_x.shape[2]),\n",
    "                   kernel_regularizer=l2(reg_learning_rate), \n",
    "                   recurrent_regularizer=l2(reg_learning_rate)))\n",
    "    \n",
    "    for i in range(1, num_lstm_layers):\n",
    "        model.add(LSTM(lstm_units, return_sequences=True if i < num_lstm_layers - 1 else False,\n",
    "                       kernel_regularizer=l2(reg_learning_rate), \n",
    "                       recurrent_regularizer=l2(reg_learning_rate)))\n",
    "        \n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(train_y.shape[1], activation='softmax', kernel_regularizer=l2(reg_learning_rate)))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', Precision(), Recall()])\n",
    "    model.fit(train_x, train_y, validation_data=(val_x, val_y) if len(val_x) != 0 else None, epochs=30, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    if len(val_x) != 0:\n",
    "        results = model.evaluate(val_x, val_y, verbose=0)\n",
    "    else:\n",
    "        results = model.evaluate(train_x, train_y, verbose=0)\n",
    "    metrics_names = model.metrics_names\n",
    "\n",
    "    accuracy = results[metrics_names.index('accuracy')]\n",
    "    precision = get_metrics(results, metrics_names, 'precision')\n",
    "    recall = get_metrics(results, metrics_names, 'recall')\n",
    "    \n",
    "    return model, accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hyperparameters: {'window_size': 280, 'dropout_rate': 0.5, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.001, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Combined Segments Shape: (1710, 281, 2)\n",
      "One-Hot Labels Shape: (1710, 5)\n",
      "Class L: 1078\n",
      "Class V: 59\n",
      "Class A: 38\n",
      "Class E: 84\n",
      "Class !: 109\n",
      "Epoch 1/30\n",
      "13/35 [==========>...................] - ETA: 3s - loss: 1.4501 - accuracy: 0.7788 - precision_1: 0.8214 - recall_1: 0.1659    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m train_x_fold, val_x_fold \u001b[38;5;241m=\u001b[39m train_x_standardised[train_index], train_x_standardised[val_index]\n\u001b[1;32m     29\u001b[0m train_y_fold, val_y_fold \u001b[38;5;241m=\u001b[39m train_y[train_index], train_y[val_index]\n\u001b[0;32m---> 31\u001b[0m model, accuracy, precision, recall \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_and_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchosen_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m     33\u001b[0m precisions\u001b[38;5;241m.\u001b[39mappend(precision)\n",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m, in \u001b[0;36mbuild_and_train_model\u001b[0;34m(train_x, train_y, dropout_rate, lstm_units, batch_size, learning_rate, reg_learning_rate, num_lstm_layers, window_size, val_x, val_y)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, Precision(), Recall()])\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     26\u001b[0m     results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(val_x, val_y, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "n_iterations = 2\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "best_test_x = []\n",
    "best_test_y = []\n",
    "best_scaler = None\n",
    "best_train_x = []\n",
    "best_train_y = []\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    chosen_params = { param: np.random.choice(values) for param, values in hyperparameter_space.items() }\n",
    "    print(f\"Current Hyperparameters: {chosen_params}\")\n",
    "\n",
    "    train_x_standardised, train_y, test_x, test_y, scaler = prepare_data(filtered_ecg_data_207, annotations_207, window_size=chosen_params['window_size'])\n",
    "\n",
    "    y_labels = np.argmax(train_y, axis=1)\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for train_index, val_index in kf.split(train_x_standardised, y_labels):\n",
    "        train_x_fold, val_x_fold = train_x_standardised[train_index], train_x_standardised[val_index]\n",
    "        train_y_fold, val_y_fold = train_y[train_index], train_y[val_index]\n",
    "\n",
    "        model, accuracy, precision, recall = build_and_train_model(train_x_fold, train_y_fold, **chosen_params, val_x=val_x_fold, val_y=val_y_fold,)\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        print(f\"Fold Scores: Acc - {accuracy} Pr - {precision} Re - {recall}\")\n",
    "    \n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "\n",
    "    print(f\"Current Mean Scores: Acc - {avg_accuracy} Pr - {avg_precision} Re - {avg_recall} , Current Hyperparameters: {chosen_params}\")\n",
    "\n",
    "    # Update best params etc.\n",
    "    if avg_accuracy > best_score:\n",
    "        best_score = avg_accuracy\n",
    "        best_params = chosen_params\n",
    "        best_metrics = {\n",
    "            'accuracy': avg_accuracy,\n",
    "            'precision': avg_precision,\n",
    "            'recall': avg_recall\n",
    "        }\n",
    "        best_test_x = test_x\n",
    "        best_test_y = test_y\n",
    "        best_scaler = scaler\n",
    "        best_train_x = train_x_standardised\n",
    "        best_train_y = train_y\n",
    "        print(f\"New best score: {avg_accuracy:.4f} with params: {best_params} and metrics: {best_metrics}\")\n",
    "\n",
    "# Final best results\n",
    "print(f\"Best score: {best_score:.4f}\")\n",
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Best metrics: {best_metrics}\")\n",
    "\n",
    "# TODO:\n",
    "# Try out balancing (weighted, or oversampling)\n",
    "# Try out diff. window sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout Rate: 0.2\n",
      "LSTM Layers: 4\n",
      "LSTM Units: 32\n",
      "Batch Size: 64\n",
      "Learning Rate: 0.001\n",
      "Regularizer Learning Rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "print(f\"Window Size: {best_params['window_size']}\")\n",
    "print(f\"Dropout Rate: {best_params['dropout_rate']}\")\n",
    "print(f\"LSTM Layers: {best_params['num_lstm_layers']}\")\n",
    "print(f\"LSTM Units: {best_params['lstm_units']}\")\n",
    "print(f\"Batch Size: {best_params['batch_size']}\")\n",
    "print(f\"Learning Rate: {best_params['learning_rate']}\")\n",
    "print(f\"Regularizer Learning Rate: {best_params['reg_learning_rate']}\")\n",
    "\n",
    "# Train final model with the best parameters\n",
    "model, accuracy, precision, recall = build_and_train_model(train_x_standardised, train_y, **best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise Test Set\n",
    "test_nsamples, test_ntimesteps, test_nfeatures = test_x.shape\n",
    "test_x_reshaped = test_x.reshape((test_nsamples * test_ntimesteps, test_nfeatures))\n",
    "test_x_standardised = scaler.transform(test_x_reshaped)\n",
    "test_x_standardised = test_x_standardised.reshape((test_nsamples, test_ntimesteps, test_nfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 46ms/step - loss: 0.3578 - accuracy: 0.9220 - precision_29: 0.9407 - recall_29: 0.9007\n",
      "Test Loss: 0.35776078701019287\n",
      "Test Accuracy: 0.9219858050346375\n",
      "Test Precision: 0.9407407641410828\n",
      "Test Recall: 0.9007092118263245\n",
      "Test F1 Score: 0.9202898620770421\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Test Set\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_x_standardised, test_y, verbose=1)\n",
    "test_f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "print(f\"Test F1 Score: {test_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 3s 46ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           L       0.96      0.98      0.97       313\n",
      "           V       0.69      0.48      0.56        23\n",
      "           A       0.54      0.47      0.50        15\n",
      "           E       0.93      0.87      0.90        30\n",
      "           !       0.86      0.90      0.88        42\n",
      "\n",
      "    accuracy                           0.92       423\n",
      "   macro avg       0.79      0.74      0.76       423\n",
      "weighted avg       0.92      0.92      0.92       423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = model.predict(test_x_standardised)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(test_y, axis=1)\n",
    "report = classification_report(true_classes, predicted_classes, target_names=['L', 'V', 'A', 'E', '!'])\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
