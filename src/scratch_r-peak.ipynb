{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x_/27ffh59s2pl353c1p0mccnzc0000gn/T/ipykernel_11930/1582698094.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_ecg_data_and_annotations(ecg_file_path, annotation_file_path):\n",
    "    ecg_data = pd.read_csv(ecg_file_path)\n",
    "    annotations = pd.read_csv(annotation_file_path)\n",
    "\n",
    "    return ecg_data, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def _butter_filter(sequence):\n",
    "    fs = 360  # Sampling frequency\n",
    "    nyquist = 0.5 * fs\n",
    "    low = 0.4 / nyquist\n",
    "    high = 45 / nyquist\n",
    "\n",
    "    b, a = butter(N=3, Wn=[low, high], btype='band')\n",
    "    return filtfilt(b, a, sequence)\n",
    "\n",
    "def apply_filter(ecg_data):\n",
    "    filtered_data = ecg_data.copy()\n",
    "    for lead in ['MLII', 'V1']:\n",
    "        # Check if the lead is in the DataFrame\n",
    "        if lead in ecg_data.columns:\n",
    "            filtered_data[lead] = _butter_filter(ecg_data[lead].values)\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def detect_r_peaks(ecg_lead, distance=180):\n",
    "    peaks, _ = find_peaks(ecg_lead, distance=distance)\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data_207, annotations_207 = load_ecg_data_and_annotations('../data/207/207.csv', '../data/207/207annotations.csv')\n",
    "filtered_ecg_data_207 = apply_filter(ecg_data_207)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  50,  296,  555,  836, 1047, 1304, 1588, 1806, 2029, 2343])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 180 # 180\n",
    "r_peaks = detect_r_peaks(filtered_ecg_data_207['MLII'].values, distance=window_size)\n",
    "r_peaks[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_data = []\n",
    "\n",
    "for r_peak in r_peaks:\n",
    "    start = max(0, r_peak - window_size // 2)\n",
    "    end = min(len(filtered_ecg_data_207), r_peak + window_size // 2)\n",
    "\n",
    "    relevant_annotations = annotations_207[(annotations_207['Sample #'] >= start) & (annotations_207['Sample #'] <= end)]\n",
    "    relevant_annotations = relevant_annotations[relevant_annotations['Type'].isin(['L', 'V', 'A', 'E', '!'])]\n",
    "\n",
    "    if not relevant_annotations.empty:\n",
    "        closest_annotation = relevant_annotations.iloc[(relevant_annotations['Sample #'] - r_peak).abs().argsort()[:1]]\n",
    "        label = closest_annotation['Type'].values[0]\n",
    "        segment_data.append({'Start': start, 'End': end, 'Label': label})\n",
    "\n",
    "segments = pd.DataFrame(segment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_data = []\n",
    "\n",
    "for r_peak in r_peaks:\n",
    "    start = max(0, r_peak - window_size // 2)\n",
    "    end = min(len(filtered_ecg_data_207), r_peak + window_size // 2)\n",
    "\n",
    "    # Closes Annotation to R-Peak\n",
    "    closest_annotation = annotations_207.iloc[(annotations_207['Sample #'] - r_peak).abs().argsort()[:1]]\n",
    "\n",
    "    if closest_annotation['Type'].values[0] in ['L', 'V', 'A', 'E', '!']:\n",
    "    \n",
    "        label = closest_annotation['Type'].values[0]\n",
    "        segment_data.append({'Start': start, 'End': end, 'Label': label})\n",
    "\n",
    "segments = pd.DataFrame(segment_data)\n",
    "segments.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove last Row since window size < 180\n",
    "segments.drop(segments.tail(1).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 11:05:39.820619: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Segments Shape: (719, 181, 2)\n",
      "One-Hot Labels Shape: (719, 5)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "segments_feature_1 = []\n",
    "segments_feature_2 = []\n",
    "segment_labels = []\n",
    "\n",
    "\n",
    "for index, row in segments.iterrows():\n",
    "    start_index = int(row['Start'])\n",
    "    end_index = int(row['End'])\n",
    "    label = row['Label']\n",
    "    \n",
    "    segment_mlII = filtered_ecg_data_207['MLII'][start_index:end_index+1].values\n",
    "    segment_v1 = filtered_ecg_data_207['V1'][start_index:end_index+1].values\n",
    "    \n",
    "    segments_feature_1.append(segment_mlII)\n",
    "    segments_feature_2.append(segment_v1)\n",
    "    segment_labels.append(label)\n",
    "\n",
    "combined_segments = [np.column_stack((mlII, v1)) for mlII, v1 in zip(segments_feature_1, segments_feature_2)]\n",
    "combined_segments_array = np.array([np.array(segment) for segment in combined_segments], dtype=object)\n",
    "\n",
    "label_mapping = {'L': 0, 'V': 1, 'A': 2, 'E': 3, '!': 4}\n",
    "integer_labels = np.array([label_mapping[label] for label in segment_labels])\n",
    "one_hot_labels = to_categorical(integer_labels)\n",
    "\n",
    "print(f\"Combined Segments Shape: {combined_segments_array.shape}\")\n",
    "print(f\"One-Hot Labels Shape: {one_hot_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train/test\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    combined_segments_array, one_hot_labels, test_size=0.2, random_state=42, stratify=one_hot_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class L: 282\n",
      "Class V: 47\n",
      "Class A: 75\n",
      "Class E: 104\n",
      "Class !: 211\n"
     ]
    }
   ],
   "source": [
    "integer_labels_from_one_hot = np.argmax(one_hot_labels, axis=1)\n",
    "\n",
    "class_counts = np.bincount(integer_labels_from_one_hot)\n",
    "class_names = ['L', 'V', 'A', 'E', '!']\n",
    "\n",
    "for class_name, count in zip(class_names, class_counts):\n",
    "    print(f\"Class {class_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Standardise Train Set\n",
    "nsamples, ntimesteps, nfeatures = train_x.shape\n",
    "train_x_reshaped = train_x.reshape((nsamples*ntimesteps, nfeatures))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x_reshaped)\n",
    "\n",
    "train_x_standardised = scaler.transform(train_x_reshaped)\n",
    "train_x_standardised = train_x_standardised.reshape((nsamples, ntimesteps, nfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(results, metrics_names, metric_key):\n",
    "    for name, value in zip(metrics_names, results):\n",
    "        if metric_key in name:\n",
    "            return value\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_space = {\n",
    "    'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "    'lstm_units': [32, 64],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'num_lstm_layers': [2, 3, 4],\n",
    "    'reg_learning_rate': [0.1, 0.01, 0.001]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_and_train_model(train_x, train_y, dropout_rate, lstm_units, batch_size, learning_rate, reg_learning_rate, num_lstm_layers, val_x=[], val_y=[]):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, return_sequences=True, input_shape=(train_x.shape[1], train_x.shape[2]),\n",
    "                   kernel_regularizer=l2(reg_learning_rate), \n",
    "                   recurrent_regularizer=l2(reg_learning_rate)))\n",
    "    \n",
    "    for i in range(1, num_lstm_layers):\n",
    "        model.add(LSTM(lstm_units, return_sequences=True if i < num_lstm_layers - 1 else False,\n",
    "                       kernel_regularizer=l2(reg_learning_rate), \n",
    "                       recurrent_regularizer=l2(reg_learning_rate)))\n",
    "        \n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(train_y.shape[1], activation='softmax', kernel_regularizer=l2(reg_learning_rate)))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', Precision(), Recall()])\n",
    "    model.fit(train_x, train_y, validation_data=(val_x, val_y) if len(val_x) != 0 else None, epochs=30, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    if len(val_x) != 0:\n",
    "        results = model.evaluate(val_x, val_y, verbose=0)\n",
    "    else:\n",
    "        results = model.evaluate(train_x, train_y, verbose=0)\n",
    "    metrics_names = model.metrics_names\n",
    "\n",
    "    accuracy = results[metrics_names.index('accuracy')]\n",
    "    precision = get_metrics(results, metrics_names, 'precision')\n",
    "    recall = get_metrics(results, metrics_names, 'recall')\n",
    "    \n",
    "    return model, accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "y_labels = np.argmax(train_y, axis=1)\n",
    "\n",
    "n_iterations = 10\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    chosen_params = { param: np.random.choice(values) for param, values in hyperparameter_space.items() }\n",
    "    print(f\"Current Hyperparameters: {chosen_params}\")\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for train_index, val_index in kf.split(train_x_standardised, y_labels):\n",
    "        train_x_fold, val_x_fold = train_x_standardised[train_index], train_x_standardised[val_index]\n",
    "        train_y_fold, val_y_fold = train_y[train_index], train_y[val_index]\n",
    "\n",
    "        model, accuracy, precision, recall = build_and_train_model(train_x_fold, train_y_fold, **chosen_params, val_x=val_x_fold, val_y=val_y_fold,)\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        print(f\"Fold Scores: Acc - {accuracy} Pr - {precision} Re - {recall}\")\n",
    "    \n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "\n",
    "    print(f\"Current Mean Scores: Acc - {avg_accuracy} Pr - {avg_precision} Re - {avg_recall} , Current Hyperparameters: {chosen_params}\")\n",
    "\n",
    "    # Update best params etc.\n",
    "    if avg_accuracy > best_score:\n",
    "        best_score = avg_accuracy\n",
    "        best_params = chosen_params\n",
    "        best_metrics = {\n",
    "            'accuracy': avg_accuracy,\n",
    "            'precision': avg_precision,\n",
    "            'recall': avg_recall\n",
    "        }\n",
    "        print(f\"New best score: {avg_accuracy:.4f} with params: {best_params} and metrics: {best_metrics}\")\n",
    "\n",
    "# Final best results\n",
    "print(f\"Best score: {best_score:.4f}\")\n",
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Best metrics: {best_metrics}\")\n",
    "\n",
    "# TODO:\n",
    "# Try out wighted classes\n",
    "# Try out diff. window sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dropout Rate: {best_params['dropout_rate']}\")\n",
    "print(f\"LSTM Layers: {best_params['num_lstm_layers']}\")\n",
    "print(f\"LSTM Units: {best_params['lstm_units']}\")\n",
    "print(f\"Batch Size: {best_params['batch_size']}\")\n",
    "print(f\"Learning Rate: {best_params['learning_rate']}\")\n",
    "print(f\"Regularizer Learning Rate: {best_params['reg_learning_rate']}\")\n",
    "\n",
    "# Train final model with the best parameters\n",
    "model, accuracy, precision, recall = build_and_train_model(train_x_standardised, train_y, **best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise Test Set\n",
    "test_nsamples, test_ntimesteps, test_nfeatures = test_x.shape\n",
    "test_x_reshaped = test_x.reshape((test_nsamples * test_ntimesteps, test_nfeatures))\n",
    "test_x_standardised = scaler.transform(test_x_reshaped)\n",
    "test_x_standardised = test_x_standardised.reshape((test_nsamples, test_ntimesteps, test_nfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 32ms/step - loss: 0.8066 - accuracy: 0.8333 - precision_188: 0.8417 - recall_188: 0.8125\n",
      "Test Loss: 0.806583821773529\n",
      "Test Accuracy: 0.8333333134651184\n",
      "Test Precision: 0.8417266011238098\n",
      "Test Recall: 0.8125\n",
      "Test F1 Score: 0.8268551151921768\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Test Set\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_x_standardised, test_y, verbose=1)\n",
    "test_f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "print(f\"Test F1 Score: {test_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 34ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           L       0.90      0.91      0.90        57\n",
      "           V       0.67      0.22      0.33         9\n",
      "           A       1.00      0.53      0.70        15\n",
      "           E       0.72      0.86      0.78        21\n",
      "           !       0.80      0.95      0.87        42\n",
      "\n",
      "    accuracy                           0.83       144\n",
      "   macro avg       0.82      0.70      0.72       144\n",
      "weighted avg       0.84      0.83      0.82       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = model.predict(test_x_standardised)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(test_y, axis=1)\n",
    "report = classification_report(true_classes, predicted_classes, target_names=['L', 'V', 'A', 'E', '!'])\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
