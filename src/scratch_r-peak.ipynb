{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_ecg_data_and_annotations(ecg_file_path, annotation_file_path):\n",
    "    ecg_data = pd.read_csv(ecg_file_path)\n",
    "    annotations = pd.read_csv(annotation_file_path)\n",
    "\n",
    "    return ecg_data, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def _butter_filter(sequence):\n",
    "    fs = 360  # Sampling frequency\n",
    "    nyquist = 0.5 * fs\n",
    "    low = 0.4 / nyquist\n",
    "    high = 45 / nyquist\n",
    "\n",
    "    b, a = butter(N=3, Wn=[low, high], btype='band')\n",
    "    return filtfilt(b, a, sequence)\n",
    "\n",
    "def apply_filter(ecg_data):\n",
    "    filtered_data = ecg_data.copy()\n",
    "    for lead in ['MLII', 'V1']:\n",
    "        # Check if the lead is in the DataFrame\n",
    "        if lead in ecg_data.columns:\n",
    "            filtered_data[lead] = _butter_filter(ecg_data[lead].values)\n",
    "\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def detect_r_peaks(ecg_lead, distance=180):\n",
    "    peaks, _ = find_peaks(ecg_lead, distance=distance)\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data_207, annotations_207 = load_ecg_data_and_annotations('../data/207/207.csv', '../data/207/207annotations.csv')\n",
    "filtered_ecg_data_207 = apply_filter(ecg_data_207)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  50,  296,  555,  836, 1047, 1304, 1588, 1806, 2029, 2343])"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 180 # 180\n",
    "r_peaks = detect_r_peaks(filtered_ecg_data_207['MLII'].values, distance=window_size)\n",
    "r_peaks[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_data = []\n",
    "\n",
    "for r_peak in r_peaks:\n",
    "    start = max(0, r_peak - window_size // 2)\n",
    "    end = min(len(filtered_ecg_data_207), r_peak + window_size // 2)\n",
    "\n",
    "    relevant_annotations = annotations_207[(annotations_207['Sample #'] >= start) & (annotations_207['Sample #'] <= end)]\n",
    "    relevant_annotations = relevant_annotations[relevant_annotations['Type'].isin(['L', 'V', 'A', 'E', '!'])]\n",
    "\n",
    "    if not relevant_annotations.empty:\n",
    "        closest_annotation = relevant_annotations.iloc[(relevant_annotations['Sample #'] - r_peak).abs().argsort()[:1]]\n",
    "        label = closest_annotation['Type'].values[0]\n",
    "        segment_data.append({'Start': start, 'End': end, 'Label': label})\n",
    "\n",
    "segments = pd.DataFrame(segment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_data = []\n",
    "\n",
    "for r_peak in r_peaks:\n",
    "    start = max(0, r_peak - window_size // 2)\n",
    "    end = min(len(filtered_ecg_data_207), r_peak + window_size // 2)\n",
    "\n",
    "    # Closes Annotation to R-Peak\n",
    "    closest_annotation = annotations_207.iloc[(annotations_207['Sample #'] - r_peak).abs().argsort()[:1]]\n",
    "\n",
    "    if closest_annotation['Type'].values[0] in ['L', 'V', 'A', 'E', '!']:\n",
    "    \n",
    "        label = closest_annotation['Type'].values[0]\n",
    "        segment_data.append({'Start': start, 'End': end, 'Label': label})\n",
    "\n",
    "segments = pd.DataFrame(segment_data)\n",
    "segments.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove last Row since window size < 180\n",
    "segments.drop(segments.tail(1).index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Segments Shape: (719, 181, 2)\n",
      "One-Hot Labels Shape: (719, 5)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "segments_feature_1 = []\n",
    "segments_feature_2 = []\n",
    "segment_labels = []\n",
    "\n",
    "\n",
    "for index, row in segments.iterrows():\n",
    "    start_index = int(row['Start'])\n",
    "    end_index = int(row['End'])\n",
    "    label = row['Label']\n",
    "    \n",
    "    segment_mlII = filtered_ecg_data_207['MLII'][start_index:end_index+1].values\n",
    "    segment_v1 = filtered_ecg_data_207['V1'][start_index:end_index+1].values\n",
    "    \n",
    "    segments_feature_1.append(segment_mlII)\n",
    "    segments_feature_2.append(segment_v1)\n",
    "    segment_labels.append(label)\n",
    "\n",
    "combined_segments = [np.column_stack((mlII, v1)) for mlII, v1 in zip(segments_feature_1, segments_feature_2)]\n",
    "combined_segments_array = np.array([np.array(segment) for segment in combined_segments], dtype=object)\n",
    "\n",
    "label_mapping = {'L': 0, 'V': 1, 'A': 2, 'E': 3, '!': 4}\n",
    "integer_labels = np.array([label_mapping[label] for label in segment_labels])\n",
    "one_hot_labels = to_categorical(integer_labels)\n",
    "\n",
    "print(f\"Combined Segments Shape: {combined_segments_array.shape}\")\n",
    "print(f\"One-Hot Labels Shape: {one_hot_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train/test\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    combined_segments_array, one_hot_labels, test_size=0.2, random_state=42, stratify=one_hot_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class L: 282\n",
      "Class V: 47\n",
      "Class A: 75\n",
      "Class E: 104\n",
      "Class !: 211\n"
     ]
    }
   ],
   "source": [
    "integer_labels_from_one_hot = np.argmax(one_hot_labels, axis=1)\n",
    "\n",
    "class_counts = np.bincount(integer_labels_from_one_hot)\n",
    "class_names = ['L', 'V', 'A', 'E', '!']\n",
    "\n",
    "for class_name, count in zip(class_names, class_counts):\n",
    "    print(f\"Class {class_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Standardise Train Set\n",
    "nsamples, ntimesteps, nfeatures = train_x.shape\n",
    "train_x_reshaped = train_x.reshape((nsamples*ntimesteps, nfeatures))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x_reshaped)\n",
    "\n",
    "train_x_standardised = scaler.transform(train_x_reshaped)\n",
    "train_x_standardised = train_x_standardised.reshape((nsamples, ntimesteps, nfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(results, metrics_names, metric_key):\n",
    "    for name, value in zip(metrics_names, results):\n",
    "        if metric_key in name:\n",
    "            return value\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_space = {\n",
    "    'dropout_rate': [0.2, 0.3, 0.4, 0.5],\n",
    "    'lstm_units': [32, 64],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'num_lstm_layers': [2, 3, 4],\n",
    "    'reg_learning_rate': [0.1, 0.01, 0.001]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def build_and_train_model(train_x, train_y, dropout_rate, lstm_units, batch_size, learning_rate, reg_learning_rate, num_lstm_layers, val_x=None, val_y=None):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, return_sequences=True, input_shape=(train_x.shape[1], train_x.shape[2]),\n",
    "                   kernel_regularizer=l2(reg_learning_rate), \n",
    "                   recurrent_regularizer=l2(reg_learning_rate)))\n",
    "    \n",
    "    for i in range(1, num_lstm_layers):\n",
    "        model.add(LSTM(lstm_units, return_sequences=True if i < num_lstm_layers - 1 else False,\n",
    "                       kernel_regularizer=l2(reg_learning_rate), \n",
    "                       recurrent_regularizer=l2(reg_learning_rate)))\n",
    "        \n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(train_y.shape[1], activation='softmax', kernel_regularizer=l2(reg_learning_rate)))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', Precision(), Recall()])\n",
    "    model.fit(train_x, train_y, validation_data=(val_x, val_y) if val_x != None else None, epochs=30, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    if val_x != None:\n",
    "        results = model.evaluate(val_x, val_y, verbose=0)\n",
    "    else:\n",
    "        results = model.evaluate(train_x, train_y, verbose=0)\n",
    "    metrics_names = model.metrics_names\n",
    "\n",
    "    accuracy = results[metrics_names.index('accuracy')]\n",
    "    precision = get_metrics(results, metrics_names, 'precision')\n",
    "    recall = get_metrics(results, metrics_names, 'recall')\n",
    "    \n",
    "    return model, accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Fold Scores: Acc - 0.886956512928009 Pr - 0.8938053250312805 Re - 0.8782608509063721 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Fold Scores: Acc - 0.8695651888847351 Pr - 0.8839285969734192 Re - 0.8608695864677429 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Fold Scores: Acc - 0.904347836971283 Pr - 0.9272727370262146 Re - 0.886956512928009 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Fold Scores: Acc - 0.8782608509063721 Pr - 0.9082568883895874 Re - 0.8608695864677429 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Fold Scores: Acc - 0.852173924446106 Pr - 0.8727272748947144 Re - 0.834782600402832 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Current Mean Scores: Acc - 0.878260862827301 Pr - 0.8971981644630432 Re - 0.8643478274345398 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "New best score: 0.8783 with params: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01} and metrics: {'accuracy': 0.878260862827301, 'precision': 0.8971981644630432, 'recall': 0.8643478274345398}\n",
      "Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Fold Scores: Acc - 0.8608695864677429 Pr - 0.8888888955116272 Re - 0.834782600402832 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Fold Scores: Acc - 0.895652174949646 Pr - 0.9189189076423645 Re - 0.886956512928009 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Fold Scores: Acc - 0.8608695864677429 Pr - 0.8608695864677429 Re - 0.8608695864677429 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Fold Scores: Acc - 0.886956512928009 Pr - 0.8981481194496155 Re - 0.843478262424469 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Fold Scores: Acc - 0.8260869383811951 Pr - 0.8230088353157043 Re - 0.8086956739425659 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Current Mean Scores: Acc - 0.8660869598388672 Pr - 0.8779668688774109 Re - 0.8469565272331238 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.001, 'num_lstm_layers': 3, 'reg_learning_rate': 0.01}\n",
      "Current Hyperparameters: {'dropout_rate': 0.5, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.5, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.5, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.5, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.5, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.2956521809101105 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.5, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Current Mean Scores: Acc - 0.3721739113330841 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.5, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.6608695387840271 Pr - 0.7362637519836426 Re - 0.582608699798584 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.5043478012084961 Pr - 0.5443037748336792 Re - 0.373913049697876 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.678260862827301 Pr - 0.723809540271759 Re - 0.6608695387840271 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.678260862827301 Pr - 0.8271604776382446 Re - 0.582608699798584 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Current Mean Scores: Acc - 0.5826086819171905 Pr - 0.5663075089454651 Re - 0.4399999976158142 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.9130434989929199 Pr - 0.9122806787490845 Re - 0.904347836971283 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.8695651888847351 Pr - 0.8839285969734192 Re - 0.8608695864677429 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.904347836971283 Pr - 0.8999999761581421 Re - 0.8608695864677429 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.8782608509063721 Pr - 0.8859649300575256 Re - 0.8782608509063721 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.852173924446106 Pr - 0.852173924446106 Re - 0.852173924446106 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Current Mean Scores: Acc - 0.8834782600402832 Pr - 0.8868696212768554 Re - 0.8713043570518494 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "New best score: 0.8835 with params: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001} and metrics: {'accuracy': 0.8834782600402832, 'precision': 0.8868696212768554, 'recall': 0.8713043570518494}\n",
      "Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 4, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.886956512928009 Pr - 0.9090909361839294 Re - 0.8695651888847351 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 4, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.6000000238418579 Pr - 0.7777777910232544 Re - 0.426086962223053 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 4, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.852173924446106 Pr - 0.8636363744735718 Re - 0.8260869383811951 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 4, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.678260862827301 Pr - 0.7052631378173828 Re - 0.582608699798584 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 4, 'reg_learning_rate': 0.001}\n",
      "Fold Scores: Acc - 0.834782600402832 Pr - 0.8421052694320679 Re - 0.834782600402832 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 4, 'reg_learning_rate': 0.001}\n",
      "Current Mean Scores: Acc - 0.7704347848892212 Pr - 0.8195747017860413 Re - 0.7078260779380798 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 4, 'reg_learning_rate': 0.001}\n",
      "Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 32, 'batch_size': 16, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.4000000059604645 Pr - 0.8823529481887817 Re - 0.1304347813129425 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 32, 'batch_size': 16, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 32, 'batch_size': 16, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 32, 'batch_size': 16, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.678260862827301 Pr - 0.7534246444702148 Re - 0.47826087474823 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 32, 'batch_size': 16, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.48695650696754456 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 32, 'batch_size': 16, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Current Mean Scores: Acc - 0.469565212726593 Pr - 0.3271555185317993 Re - 0.1217391312122345 , Current Hyperparameters: {'dropout_rate': 0.2, 'lstm_units': 32, 'batch_size': 16, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.1}\n",
      "Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 32, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 4, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 32, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 4, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 32, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 4, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 32, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 4, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 32, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 4, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 32, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 4, 'reg_learning_rate': 0.1}\n",
      "Current Mean Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 32, 'batch_size': 32, 'learning_rate': 0.1, 'num_lstm_layers': 4, 'reg_learning_rate': 0.1}\n",
      "Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Current Mean Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.3, 'lstm_units': 64, 'batch_size': 64, 'learning_rate': 0.01, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.2956521809101105 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Fold Scores: Acc - 0.3913043439388275 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Current Mean Scores: Acc - 0.3721739113330841 Pr - 0.0 Re - 0.0 , Current Hyperparameters: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 16, 'learning_rate': 0.1, 'num_lstm_layers': 3, 'reg_learning_rate': 0.1}\n",
      "Best score: 0.8835\n",
      "Best params: {'dropout_rate': 0.4, 'lstm_units': 64, 'batch_size': 32, 'learning_rate': 0.01, 'num_lstm_layers': 2, 'reg_learning_rate': 0.001}\n",
      "Best metrics: {'accuracy': 0.8834782600402832, 'precision': 0.8868696212768554, 'recall': 0.8713043570518494}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "y_labels = np.argmax(train_y, axis=1)\n",
    "\n",
    "n_iterations = 10\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    chosen_params = { param: np.random.choice(values) for param, values in hyperparameter_space.items() }\n",
    "    print(f\"Current Hyperparameters: {chosen_params}\")\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for train_index, val_index in kf.split(train_x_standardised, y_labels):\n",
    "        train_x_fold, val_x_fold = train_x_standardised[train_index], train_x_standardised[val_index]\n",
    "        train_y_fold, val_y_fold = train_y[train_index], train_y[val_index]\n",
    "\n",
    "        model, accuracy, precision, recall = build_and_train_model(train_x_fold, train_y_fold, val_x_fold, val_y_fold, **chosen_params)\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        print(f\"Fold Scores: Acc - {accuracy} Pr - {precision} Re - {recall}\")\n",
    "    \n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "\n",
    "    print(f\"Current Mean Scores: Acc - {avg_accuracy} Pr - {avg_precision} Re - {avg_recall} , Current Hyperparameters: {chosen_params}\")\n",
    "\n",
    "    # Update best params etc.\n",
    "    if avg_accuracy > best_score:\n",
    "        best_score = avg_accuracy\n",
    "        best_params = chosen_params\n",
    "        best_metrics = {\n",
    "            'accuracy': avg_accuracy,\n",
    "            'precision': avg_precision,\n",
    "            'recall': avg_recall\n",
    "        }\n",
    "        print(f\"New best score: {avg_accuracy:.4f} with params: {best_params} and metrics: {best_metrics}\")\n",
    "\n",
    "# Final best results\n",
    "print(f\"Best score: {best_score:.4f}\")\n",
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Best metrics: {best_metrics}\")\n",
    "\n",
    "# TODO:\n",
    "# Try out wighted classes\n",
    "# Try out diff. window sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dropout Rate: {best_params['dropout_rate']}\")\n",
    "print(f\"LSTM Layers: {best_params['num_lstm_layers']}\")\n",
    "print(f\"LSTM Units: {best_params['lstm_units']}\")\n",
    "print(f\"Batch Size: {best_params['batch_size']}\")\n",
    "print(f\"Learning Rate: {best_params['learning_rate']}\")\n",
    "print(f\"Regularizer Learning Rate: {best_params['reg_learning_rate']}\")\n",
    "\n",
    "# Train final model with the best parameters\n",
    "model, accuracy, precision, recall = build_and_train_model(train_x_standardised, train_y, **best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise Test Set\n",
    "test_nsamples, test_ntimesteps, test_nfeatures = test_x.shape\n",
    "test_x_reshaped = test_x.reshape((test_nsamples * test_ntimesteps, test_nfeatures))\n",
    "test_x_standardised = scaler.transform(test_x_reshaped)\n",
    "test_x_standardised = test_x_standardised.reshape((test_nsamples, test_ntimesteps, test_nfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 32ms/step - loss: 0.8066 - accuracy: 0.8333 - precision_188: 0.8417 - recall_188: 0.8125\n",
      "Test Loss: 0.806583821773529\n",
      "Test Accuracy: 0.8333333134651184\n",
      "Test Precision: 0.8417266011238098\n",
      "Test Recall: 0.8125\n",
      "Test F1 Score: 0.8268551151921768\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Test Set\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_x_standardised, test_y, verbose=1)\n",
    "test_f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "print(f\"Test F1 Score: {test_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 34ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           L       0.90      0.91      0.90        57\n",
      "           V       0.67      0.22      0.33         9\n",
      "           A       1.00      0.53      0.70        15\n",
      "           E       0.72      0.86      0.78        21\n",
      "           !       0.80      0.95      0.87        42\n",
      "\n",
      "    accuracy                           0.83       144\n",
      "   macro avg       0.82      0.70      0.72       144\n",
      "weighted avg       0.84      0.83      0.82       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = model.predict(test_x_standardised)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(test_y, axis=1)\n",
    "report = classification_report(true_classes, predicted_classes, target_names=['L', 'V', 'A', 'E', '!'])\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
